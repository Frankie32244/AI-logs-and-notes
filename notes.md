## 1.transformer 

* 如何通俗地理解transformer?
![098](https://github.com/Frankie32244/MVC-model-running-logs/blob/main/Pics/098.PNG)

* [illustrated-transformer-（一篇如何更好地理解transformer的blog）](https://jalammar.github.io/illustrated-transformer/)
* [paper-《Attention Is All You Need》from google brain](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
* [ Attention is All You Need浅读（简介+代码) ](https://kexue.fm/archives/4765)
* [知乎回答-如何更通俗地理解transformer](https://www.zhihu.com/question/445556653/answer/3254012065)

## 2.LLMs
* 《高级软件工程》Ppt LLms-lee ppt本级路径(F:\academic-reports\2023-11-13高级软件工程汇报ppt), 加上组会成员的一篇ppt，更加了解了大语言模型。
## 3.循环神经网络（RNN）和卷积神经网络（CNN)

## 4.多模态

## 5.多组学

## 6.什么是注意力机制
* 当我们处理一段文字或一组序列数据时，自注意力机制可以帮助模型更聪明地关注输入中不同位置的信息。这就好像我们在阅读一段文字时，有时会根据上下文关系有选择地关注不同的词语，而不是一成不变地按照顺序一个一个读下去。（我的理解就是大模型能够利用自注意力机制，做到一目十行，而且针对性地看不同的词语）。
## 7.encoder & decoder (编码器 & 解码器)

## 8.单细胞

## 9.推荐系统 


## 10.LSTM 

## 11.特征点以及特征点提取


## 12.GCN  